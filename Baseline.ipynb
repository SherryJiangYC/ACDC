{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import pandas as pd \n",
    "\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import PIL\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import  Conv3D, MaxPooling3D, Dropout, Dense, Input, Flatten\n",
    "from keras.layers.advanced_activations import LeakyReLU\n",
    "from keras import initializers\n",
    "from keras.regularizers import l2\n",
    "from keras.models import Model\n",
    "from keras.layers import Reshape, merge, Lambda\n",
    "from keras.layers.merge import Average\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "from keras.optimizers import SGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_patient(name):\n",
    "    imglist=[]\n",
    "    for f in os.listdir('./dataset/'+name+'/')[:8]:\n",
    "        img = PIL.Image.open('./dataset/'+name+'/{}'.format(f))\n",
    "        img = img.resize((8,8),PIL.Image.ANTIALIAS)\n",
    "        img = np.matrix(img)\n",
    "        imglist.append(img)\n",
    "    imglist = np.array(imglist)\n",
    "    return imglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patient = os.listdir('./dataset/')\n",
    "patientList=[]\n",
    "for name in patient:\n",
    "    imglist = load_patient(name)\n",
    "    patientList.append(imglist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "patientArray = np.array(patientList)\n",
    "patientArray = patientArray.reshape(100,8,8,8,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "lables = pd.read_csv('./lables.csv')\n",
    "groups = lables['Groups']\n",
    "\n",
    "lb = LabelBinarizer()\n",
    "y = lb.fit_transform(groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#LightNet\n",
    "\n",
    "cfg = {'batch_size' : 32, #52, 47892 samples, 921batches\n",
    "       # 'learning_rate' : lr_schedule,\n",
    "       'momentum' : 0.9,\n",
    "       'dims' : (8, 8, 8),\n",
    "       'n_channels' : 1,\n",
    "       'n_classes' : 5,\n",
    "       'batches_per_chunk': 1,#64 -default\n",
    "       'max_epochs' : 400,# 80 -default\n",
    "       'max_jitter_ij' : 2,\n",
    "       'max_jitter_k' : 2,\n",
    "       'n_rotations' : 12,\n",
    "       'checkpoint_every_nth' : 4000,\n",
    "       }\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_LightNet():\n",
    "    dims, n_channels, n_classes = tuple(cfg['dims']), cfg['n_channels'], cfg['n_classes']\n",
    "    n_rotations = cfg['n_rotations']\n",
    "    #tf(samples,rows,clos,height,channels)\n",
    "    shape = dims + (n_channels,)\n",
    "\n",
    "    l_in = Input(shape =shape)\n",
    "    l_conv1 = Conv3D(\n",
    "                            filters = 32,\n",
    "                            kernel_size=(2, 2, 2),\n",
    "                            padding = 'valid',\n",
    "                            kernel_initializer = 'he_normal',\n",
    "                            kernel_regularizer=l2(0.001),\n",
    "                            name = 'conv1'\n",
    "                            )(l_in)\n",
    "\n",
    "    l_conv1 = BatchNormalization()(l_conv1)\n",
    "\n",
    "    l_out1 = LeakyReLU(alpha = 0.1)(l_conv1)\n",
    "\n",
    "    l_drop1 = Dropout(rate= 0.1, name = 'drop1')(l_out1)\n",
    "\n",
    "    l_conv2 = Conv3D(\n",
    "                            filters=32,# change to 64\n",
    "                            kernel_size=(2, 2, 2),\n",
    "                            padding='valid',\n",
    "                            kernel_initializer='he_normal',\n",
    "                            kernel_regularizer=l2(0.001),\n",
    "                            name='conv2'\n",
    "                            )(l_drop1)\n",
    "\n",
    "    l_conv2 = BatchNormalization()(l_conv2)\n",
    "\n",
    "    l_out2 = LeakyReLU(alpha = 0.1)(l_conv2)\n",
    "\n",
    "    l_pool2 = MaxPooling3D(pool_size = (2, 2, 2), name = 'pool2')(l_out2)\n",
    "\n",
    "    l_drop2 = Dropout(rate = 0.1, name = 'drop2')(l_pool2)\n",
    "\n",
    "    #add extra layer compared to original VoxNet\n",
    "\n",
    "    num_of_subvolumes = 64\n",
    "    l_conv3 = Conv3D(\n",
    "        filters=num_of_subvolumes,#we put more kernels here\n",
    "        kernel_size=(2,2,2),\n",
    "        padding='valid',\n",
    "        kernel_initializer='he_normal',\n",
    "        kernel_regularizer=l2(0.001),\n",
    "        name='conv3'\n",
    "    )(l_drop2)\n",
    "\n",
    "    l_conv3 = BatchNormalization()(l_conv3)\n",
    "\n",
    "    l_out3 = LeakyReLU(alpha=0.1)(l_conv3)\n",
    "\n",
    "   # l_pool3 = MaxPooling3D(pool_size=(2, 2, 2), name='pool3')(l_out3)\n",
    "\n",
    "    l_drop3 = Dropout(rate=0.1, name='drop3')(l_out3)\n",
    "    #end\n",
    "\n",
    "    #main branch\n",
    "    l_flatten = Flatten()(l_drop3)\n",
    "\n",
    "    l_fc1 = Dense(units = num_of_subvolumes, kernel_initializer = 'he_normal', activation = 'relu', kernel_regularizer=l2(0.001), name = 'fc1')(l_flatten)\n",
    "\n",
    "    l_drop4 = Dropout(rate = 0.1, name = 'drop4' )(l_fc1)\n",
    "\n",
    "    main_classification_output = Dense(units = n_classes, kernel_initializer = 'he_normal', activation = 'softmax', kernel_regularizer=l2(0.001), name = 'main_output_40')(l_drop4)\n",
    "    # main_orientation_output = Dense(units = n_rotations, kernel_initializer = 'he_normal', activation = 'softmax', kernel_regularizer=l2(0.001), name = 'ORN_main_output')(l_drop4)\n",
    "\n",
    "\n",
    "    #auxiliary supervision\n",
    "\n",
    "    slicing = Reshape(target_shape = (8,num_of_subvolumes))(l_drop3)\n",
    "\n",
    "    part0 = Lambda(lambda x: x[:, 0, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop0 = Dropout(rate=0.2, name='subdrop0')(part0)\n",
    "    sub_output0 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output0')(l_subdrop0)\n",
    "    # ORN_output0 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output0')(l_subdrop0)\n",
    "\n",
    "    part1 = Lambda(lambda x: x[:, 1, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop1 = Dropout(rate=0.1, name='subdrop1')(part1)\n",
    "    sub_output1 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output1')(l_subdrop1)\n",
    "    # ORN_output1 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output1')(l_subdrop1)\n",
    "\n",
    "    part2 = Lambda(lambda x: x[:, 2, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop2 = Dropout(rate=0.1, name='subdrop2')(part2)\n",
    "    sub_output2 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output2')(l_subdrop2)\n",
    "    # ORN_output2 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output2')(l_subdrop2)\n",
    "\n",
    "    part3 = Lambda(lambda x: x[:, 3, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop3 = Dropout(rate=0.1, name='subdrop3')(part3)\n",
    "    sub_output3 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output3')(l_subdrop3)\n",
    "    # ORN_output3 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output3')(l_subdrop3)\n",
    "\n",
    "    part4 = Lambda(lambda x: x[:, 4, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop4 = Dropout(rate=0.1, name='subdrop4')(part4)\n",
    "    sub_output4 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output4')(l_subdrop4)\n",
    "    # ORN_output4 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output4')(l_subdrop4)\n",
    "\n",
    "\n",
    "    part5 = Lambda(lambda x: x[:, 5, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop5 = Dropout(rate=0.2, name='subdrop5')(part5)\n",
    "    sub_output5 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output5')(l_subdrop5)\n",
    "    # ORN_output5 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output5')(l_subdrop5)\n",
    "\n",
    "\n",
    "    part6 = Lambda(lambda x: x[:, 6, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop6 = Dropout(rate=0.1, name='subdrop6')(part6)\n",
    "    sub_output6 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output6')(l_subdrop6)\n",
    "    # ORN_output6 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output6')(l_subdrop6)\n",
    "\n",
    "    part7 = Lambda(lambda x: x[:, 7, :], output_shape=(num_of_subvolumes,))(slicing)\n",
    "    l_subdrop7 = Dropout(rate=0.1, name='subdrop7')(part7)\n",
    "    sub_output7 = Dense(units=n_classes, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_sub_output7')(l_subdrop7)\n",
    "    # ORN_output7 = Dense(units=n_rotations, kernel_initializer='he_normal', activation='softmax', kernel_regularizer=l2(0.001),name = 'ft_ORN_output7')(l_subdrop7)\n",
    "\n",
    "\n",
    "    #with auxiliary average, we actually give each sub_main_output a weight of 1/8 /2 = 1/16\n",
    "    #change pooling to max\n",
    "\n",
    "    classification_auxiliary_output = Average(name = 'classification_auxiliary_output')([sub_output0, sub_output1, sub_output2, sub_output3, sub_output4, sub_output5, sub_output6, sub_output7])\n",
    "\n",
    "    classification_output = Average(name = 'classification_output')([main_classification_output,classification_auxiliary_output])\n",
    "\n",
    "    #the same to Orientation\n",
    "\n",
    "    # orientation_auxiliary_output = Average( name = 'orientation_auxiliary_output')([ORN_output0, ORN_output1, ORN_output2, ORN_output3, ORN_output4, ORN_output5, ORN_output6, ORN_output7])\n",
    "    # orientation_output = Average(name = 'orientation_output' )([main_orientation_output,orientation_auxiliary_output])\n",
    "    model = Model(inputs = l_in, outputs =classification_output)\n",
    "    print('Create the Neural Net!')\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Create the Neural Net!\n"
     ]
    }
   ],
   "source": [
    "model = get_LightNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_1 (InputLayer)             (None, 8, 8, 8, 1)    0                                            \n",
      "____________________________________________________________________________________________________\n",
      "conv1 (Conv3D)                   (None, 7, 7, 7, 32)   288         input_1[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_1 (BatchNorm (None, 7, 7, 7, 32)   128         conv1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_1 (LeakyReLU)        (None, 7, 7, 7, 32)   0           batch_normalization_1[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "drop1 (Dropout)                  (None, 7, 7, 7, 32)   0           leaky_re_lu_1[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "conv2 (Conv3D)                   (None, 6, 6, 6, 32)   8224        drop1[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_2 (BatchNorm (None, 6, 6, 6, 32)   128         conv2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_2 (LeakyReLU)        (None, 6, 6, 6, 32)   0           batch_normalization_2[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "pool2 (MaxPooling3D)             (None, 3, 3, 3, 32)   0           leaky_re_lu_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "drop2 (Dropout)                  (None, 3, 3, 3, 32)   0           pool2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "conv3 (Conv3D)                   (None, 2, 2, 2, 64)   16448       drop2[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "batch_normalization_3 (BatchNorm (None, 2, 2, 2, 64)   256         conv3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "leaky_re_lu_3 (LeakyReLU)        (None, 2, 2, 2, 64)   0           batch_normalization_3[0][0]      \n",
      "____________________________________________________________________________________________________\n",
      "drop3 (Dropout)                  (None, 2, 2, 2, 64)   0           leaky_re_lu_3[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "reshape_1 (Reshape)              (None, 8, 64)         0           drop3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)              (None, 512)           0           drop3[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "lambda_1 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_2 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_3 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_5 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_7 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "lambda_8 (Lambda)                (None, 64)            0           reshape_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "fc1 (Dense)                      (None, 64)            32832       flatten_1[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "subdrop0 (Dropout)               (None, 64)            0           lambda_1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop1 (Dropout)               (None, 64)            0           lambda_2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop2 (Dropout)               (None, 64)            0           lambda_3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop3 (Dropout)               (None, 64)            0           lambda_4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop4 (Dropout)               (None, 64)            0           lambda_5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop5 (Dropout)               (None, 64)            0           lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop6 (Dropout)               (None, 64)            0           lambda_7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "subdrop7 (Dropout)               (None, 64)            0           lambda_8[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "drop4 (Dropout)                  (None, 64)            0           fc1[0][0]                        \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output0 (Dense)           (None, 5)             325         subdrop0[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output1 (Dense)           (None, 5)             325         subdrop1[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output2 (Dense)           (None, 5)             325         subdrop2[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output3 (Dense)           (None, 5)             325         subdrop3[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output4 (Dense)           (None, 5)             325         subdrop4[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output5 (Dense)           (None, 5)             325         subdrop5[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output6 (Dense)           (None, 5)             325         subdrop6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "ft_sub_output7 (Dense)           (None, 5)             325         subdrop7[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "main_output_40 (Dense)           (None, 5)             325         drop4[0][0]                      \n",
      "____________________________________________________________________________________________________\n",
      "classification_auxiliary_output  (None, 5)             0           ft_sub_output0[0][0]             \n",
      "                                                                   ft_sub_output1[0][0]             \n",
      "                                                                   ft_sub_output2[0][0]             \n",
      "                                                                   ft_sub_output3[0][0]             \n",
      "                                                                   ft_sub_output4[0][0]             \n",
      "                                                                   ft_sub_output5[0][0]             \n",
      "                                                                   ft_sub_output6[0][0]             \n",
      "                                                                   ft_sub_output7[0][0]             \n",
      "____________________________________________________________________________________________________\n",
      "classification_output (Average)  (None, 5)             0           main_output_40[0][0]             \n",
      "                                                                   classification_auxiliary_output[0\n",
      "====================================================================================================\n",
      "Total params: 61,229\n",
      "Trainable params: 60,973\n",
      "Non-trainable params: 256\n",
      "____________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.001, momentum=cfg['momentum'], decay=0.0, nesterov=True)\n",
    "model.compile(loss='categorical_crossentropy', optimizer=sgd, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "100/100 [==============================] - 3s - loss: 2.0638 - acc: 0.2400     \n",
      "Epoch 2/100\n",
      "100/100 [==============================] - 1s - loss: 1.9991 - acc: 0.2400     \n",
      "Epoch 3/100\n",
      "100/100 [==============================] - 1s - loss: 2.0477 - acc: 0.1800     \n",
      "Epoch 4/100\n",
      "100/100 [==============================] - 1s - loss: 2.0027 - acc: 0.2800     \n",
      "Epoch 5/100\n",
      "100/100 [==============================] - 1s - loss: 1.9568 - acc: 0.2500     \n",
      "Epoch 6/100\n",
      "100/100 [==============================] - 1s - loss: 1.9825 - acc: 0.2300     \n",
      "Epoch 7/100\n",
      "100/100 [==============================] - 1s - loss: 1.9774 - acc: 0.2900     \n",
      "Epoch 8/100\n",
      "100/100 [==============================] - 1s - loss: 1.9957 - acc: 0.2500     \n",
      "Epoch 9/100\n",
      "100/100 [==============================] - 1s - loss: 1.9274 - acc: 0.3000     \n",
      "Epoch 10/100\n",
      "100/100 [==============================] - 1s - loss: 1.9365 - acc: 0.3000     \n",
      "Epoch 11/100\n",
      "100/100 [==============================] - 2s - loss: 1.9176 - acc: 0.3000     \n",
      "Epoch 12/100\n",
      "100/100 [==============================] - 2s - loss: 1.9176 - acc: 0.3700     \n",
      "Epoch 13/100\n",
      "100/100 [==============================] - 2s - loss: 1.9094 - acc: 0.3400     \n",
      "Epoch 14/100\n",
      "100/100 [==============================] - 2s - loss: 1.9051 - acc: 0.3300     \n",
      "Epoch 15/100\n",
      "100/100 [==============================] - 2s - loss: 1.8614 - acc: 0.3400     \n",
      "Epoch 16/100\n",
      "100/100 [==============================] - 2s - loss: 1.9024 - acc: 0.3600     \n",
      "Epoch 17/100\n",
      "100/100 [==============================] - 2s - loss: 1.8883 - acc: 0.3500     \n",
      "Epoch 18/100\n",
      "100/100 [==============================] - 2s - loss: 1.9191 - acc: 0.3300     \n",
      "Epoch 19/100\n",
      "100/100 [==============================] - 2s - loss: 1.8761 - acc: 0.3800     \n",
      "Epoch 20/100\n",
      "100/100 [==============================] - 2s - loss: 1.8416 - acc: 0.3600     \n",
      "Epoch 21/100\n",
      "100/100 [==============================] - 2s - loss: 1.8931 - acc: 0.3500     \n",
      "Epoch 22/100\n",
      "100/100 [==============================] - 3s - loss: 1.8547 - acc: 0.3500     \n",
      "Epoch 23/100\n",
      "100/100 [==============================] - 2s - loss: 1.8603 - acc: 0.3700     \n",
      "Epoch 24/100\n",
      "100/100 [==============================] - 2s - loss: 1.8580 - acc: 0.3700     \n",
      "Epoch 25/100\n",
      "100/100 [==============================] - 2s - loss: 1.8389 - acc: 0.3700     \n",
      "Epoch 26/100\n",
      "100/100 [==============================] - 2s - loss: 1.8379 - acc: 0.3400     \n",
      "Epoch 27/100\n",
      "100/100 [==============================] - 2s - loss: 1.8195 - acc: 0.3800     \n",
      "Epoch 28/100\n",
      "100/100 [==============================] - 3s - loss: 1.8303 - acc: 0.4000     \n",
      "Epoch 29/100\n",
      "100/100 [==============================] - 2s - loss: 1.8257 - acc: 0.3700     \n",
      "Epoch 30/100\n",
      "100/100 [==============================] - 3s - loss: 1.8235 - acc: 0.3700     \n",
      "Epoch 31/100\n",
      "100/100 [==============================] - 2s - loss: 1.8363 - acc: 0.3600     \n",
      "Epoch 32/100\n",
      "100/100 [==============================] - 2s - loss: 1.8023 - acc: 0.4400     \n",
      "Epoch 33/100\n",
      "100/100 [==============================] - 3s - loss: 1.8202 - acc: 0.4100     \n",
      "Epoch 34/100\n",
      "100/100 [==============================] - 3s - loss: 1.8184 - acc: 0.4300     \n",
      "Epoch 35/100\n",
      "100/100 [==============================] - 2s - loss: 1.8285 - acc: 0.3500     \n",
      "Epoch 36/100\n",
      "100/100 [==============================] - 2s - loss: 1.8098 - acc: 0.3300     \n",
      "Epoch 37/100\n",
      "100/100 [==============================] - 2s - loss: 1.7991 - acc: 0.3600     \n",
      "Epoch 38/100\n",
      "100/100 [==============================] - 2s - loss: 1.7859 - acc: 0.4500     \n",
      "Epoch 39/100\n",
      "100/100 [==============================] - 2s - loss: 1.7754 - acc: 0.5000     \n",
      "Epoch 40/100\n",
      "100/100 [==============================] - 2s - loss: 1.7471 - acc: 0.5000     \n",
      "Epoch 41/100\n",
      "100/100 [==============================] - 2s - loss: 1.8008 - acc: 0.4000     \n",
      "Epoch 42/100\n",
      "100/100 [==============================] - 2s - loss: 1.7726 - acc: 0.4100     \n",
      "Epoch 43/100\n",
      "100/100 [==============================] - 2s - loss: 1.7320 - acc: 0.4600     \n",
      "Epoch 44/100\n",
      "100/100 [==============================] - 2s - loss: 1.7794 - acc: 0.3800     \n",
      "Epoch 45/100\n",
      "100/100 [==============================] - 2s - loss: 1.7424 - acc: 0.4300     \n",
      "Epoch 46/100\n",
      "100/100 [==============================] - 3s - loss: 1.7537 - acc: 0.4700     \n",
      "Epoch 47/100\n",
      "100/100 [==============================] - 3s - loss: 1.7591 - acc: 0.4200     \n",
      "Epoch 48/100\n",
      "100/100 [==============================] - 4s - loss: 1.7240 - acc: 0.4700     \n",
      "Epoch 49/100\n",
      "100/100 [==============================] - 3s - loss: 1.7657 - acc: 0.4400     \n",
      "Epoch 50/100\n",
      "100/100 [==============================] - 3s - loss: 1.7398 - acc: 0.4800     \n",
      "Epoch 51/100\n",
      "100/100 [==============================] - 3s - loss: 1.6945 - acc: 0.4900     \n",
      "Epoch 52/100\n",
      "100/100 [==============================] - 2s - loss: 1.7121 - acc: 0.5100     \n",
      "Epoch 53/100\n",
      "100/100 [==============================] - 2s - loss: 1.7207 - acc: 0.5100     \n",
      "Epoch 54/100\n",
      "100/100 [==============================] - 2s - loss: 1.7062 - acc: 0.5400     \n",
      "Epoch 55/100\n",
      "100/100 [==============================] - 2s - loss: 1.7438 - acc: 0.4400     \n",
      "Epoch 56/100\n",
      "100/100 [==============================] - 2s - loss: 1.6816 - acc: 0.5800     \n",
      "Epoch 57/100\n",
      "100/100 [==============================] - 2s - loss: 1.7170 - acc: 0.6100     \n",
      "Epoch 58/100\n",
      "100/100 [==============================] - 2s - loss: 1.6717 - acc: 0.5900     \n",
      "Epoch 59/100\n",
      "100/100 [==============================] - 2s - loss: 1.6560 - acc: 0.4900     \n",
      "Epoch 60/100\n",
      "100/100 [==============================] - 3s - loss: 1.6521 - acc: 0.5000     \n",
      "Epoch 61/100\n",
      "100/100 [==============================] - 2s - loss: 1.6360 - acc: 0.5800     \n",
      "Epoch 62/100\n",
      "100/100 [==============================] - 2s - loss: 1.7066 - acc: 0.4600     \n",
      "Epoch 63/100\n",
      "100/100 [==============================] - 2s - loss: 1.6781 - acc: 0.5600     \n",
      "Epoch 64/100\n",
      "100/100 [==============================] - 2s - loss: 1.6721 - acc: 0.5300     \n",
      "Epoch 65/100\n",
      "100/100 [==============================] - 2s - loss: 1.6517 - acc: 0.5800     \n",
      "Epoch 66/100\n",
      "100/100 [==============================] - 2s - loss: 1.6622 - acc: 0.5400     \n",
      "Epoch 67/100\n",
      "100/100 [==============================] - 2s - loss: 1.6249 - acc: 0.5800     \n",
      "Epoch 68/100\n",
      "100/100 [==============================] - 2s - loss: 1.6086 - acc: 0.6200     \n",
      "Epoch 69/100\n",
      "100/100 [==============================] - 2s - loss: 1.6200 - acc: 0.5700     \n",
      "Epoch 70/100\n",
      "100/100 [==============================] - 2s - loss: 1.6464 - acc: 0.5300     \n",
      "Epoch 71/100\n",
      "100/100 [==============================] - 2s - loss: 1.6261 - acc: 0.5400     \n",
      "Epoch 72/100\n",
      "100/100 [==============================] - 2s - loss: 1.6008 - acc: 0.5900     \n",
      "Epoch 73/100\n",
      "100/100 [==============================] - 2s - loss: 1.6547 - acc: 0.5800     \n",
      "Epoch 74/100\n",
      "100/100 [==============================] - 1s - loss: 1.6051 - acc: 0.5900     \n",
      "Epoch 75/100\n",
      "100/100 [==============================] - 2s - loss: 1.6013 - acc: 0.5900     \n",
      "Epoch 76/100\n",
      "100/100 [==============================] - 2s - loss: 1.6299 - acc: 0.5700     \n",
      "Epoch 77/100\n",
      "100/100 [==============================] - 2s - loss: 1.6137 - acc: 0.5700     \n",
      "Epoch 78/100\n",
      "100/100 [==============================] - 2s - loss: 1.6116 - acc: 0.6000     \n",
      "Epoch 79/100\n",
      "100/100 [==============================] - 2s - loss: 1.6097 - acc: 0.5700     \n",
      "Epoch 80/100\n",
      "100/100 [==============================] - 3s - loss: 1.6216 - acc: 0.6000     \n",
      "Epoch 81/100\n",
      "100/100 [==============================] - 2s - loss: 1.5788 - acc: 0.6200     \n",
      "Epoch 82/100\n",
      "100/100 [==============================] - 2s - loss: 1.6032 - acc: 0.5800     \n",
      "Epoch 83/100\n",
      "100/100 [==============================] - 2s - loss: 1.5612 - acc: 0.6600     \n",
      "Epoch 84/100\n",
      "100/100 [==============================] - 3s - loss: 1.5715 - acc: 0.6000     \n",
      "Epoch 85/100\n",
      "100/100 [==============================] - 2s - loss: 1.5405 - acc: 0.6700     \n",
      "Epoch 86/100\n",
      "100/100 [==============================] - 3s - loss: 1.6024 - acc: 0.6200     \n",
      "Epoch 87/100\n",
      "100/100 [==============================] - 2s - loss: 1.5605 - acc: 0.6300     \n",
      "Epoch 88/100\n",
      "100/100 [==============================] - 2s - loss: 1.5347 - acc: 0.6400     \n",
      "Epoch 89/100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100/100 [==============================] - 2s - loss: 1.5834 - acc: 0.5600     \n",
      "Epoch 90/100\n",
      "100/100 [==============================] - 1s - loss: 1.5155 - acc: 0.6700     \n",
      "Epoch 91/100\n",
      "100/100 [==============================] - 2s - loss: 1.5190 - acc: 0.6900     \n",
      "Epoch 92/100\n",
      "100/100 [==============================] - 1s - loss: 1.5746 - acc: 0.5800     \n",
      "Epoch 93/100\n",
      "100/100 [==============================] - 1s - loss: 1.5704 - acc: 0.6500     \n",
      "Epoch 94/100\n",
      "100/100 [==============================] - 1s - loss: 1.5358 - acc: 0.6400     \n",
      "Epoch 95/100\n",
      "100/100 [==============================] - 1s - loss: 1.5034 - acc: 0.6300     \n",
      "Epoch 96/100\n",
      "100/100 [==============================] - 1s - loss: 1.5013 - acc: 0.6700     \n",
      "Epoch 97/100\n",
      "100/100 [==============================] - 1s - loss: 1.5363 - acc: 0.6400     \n",
      "Epoch 98/100\n",
      "100/100 [==============================] - 1s - loss: 1.5415 - acc: 0.6500     \n",
      "Epoch 99/100\n",
      "100/100 [==============================] - 1s - loss: 1.5276 - acc: 0.6600     \n",
      "Epoch 100/100\n",
      "100/100 [==============================] - 1s - loss: 1.4774 - acc: 0.7200     \n"
     ]
    }
   ],
   "source": [
    "trainModel = model.fit(patientArray,y,batch_size=32,epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation(X,y):\n",
    "    from sklearn import cross_validation\n",
    "    k_fold = cross_validation.KFold(len(patientArray), n_folds=10)\n",
    "    \n",
    "    scorelist = []\n",
    "    for train_indices, test_indices in k_fold:\n",
    "        print('Train: %s | test: %s' % (train_indices, test_indices))\n",
    "        model.fit(X[train_indices],y[train_indices],batch_size=32,epochs=1)\n",
    "        score = model.evaluate(X[test_indices],y[test_indices])\n",
    "        scorelist.append(score)\n",
    "    result = np.mean(scorelist, axis=0)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: [10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [0 1 2 3 4 5 6 7 8 9]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.4378 - acc: 0.7111     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [10 11 12 13 14 15 16 17 18 19]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.3728 - acc: 0.8000     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 30 31 32 33 34\n",
      " 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [20 21 22 23 24 25 26 27 28 29]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.4330 - acc: 0.7222     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [30 31 32 33 34 35 36 37 38 39]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.3911 - acc: 0.7111     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 50 51 52 53 54 55 56 57 58 59\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [40 41 42 43 44 45 46 47 48 49]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.4881 - acc: 0.6222     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [50 51 52 53 54 55 56 57 58 59]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 2s - loss: 1.4742 - acc: 0.6556     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [60 61 62 63 64 65 66 67 68 69]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.4115 - acc: 0.6778     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 80 81 82 83 84\n",
      " 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99] | test: [70 71 72 73 74 75 76 77 78 79]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 2s - loss: 1.3893 - acc: 0.7111     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      " 75 76 77 78 79 90 91 92 93 94 95 96 97 98 99] | test: [80 81 82 83 84 85 86 87 88 89]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 1s - loss: 1.4326 - acc: 0.7000     \n",
      "10/10 [==============================] - 0s\n",
      "Train: [ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24\n",
      " 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49\n",
      " 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74\n",
      " 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89] | test: [90 91 92 93 94 95 96 97 98 99]\n",
      "Epoch 1/1\n",
      "90/90 [==============================] - 2s - loss: 1.4296 - acc: 0.6444     \n",
      "10/10 [==============================] - 0s\n",
      "[ 1.36530721  0.75999999]\n"
     ]
    }
   ],
   "source": [
    "result = cross_validation(patientArray,y)\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
